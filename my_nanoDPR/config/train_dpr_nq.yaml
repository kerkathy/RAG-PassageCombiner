## data
train_file: /home/guest/r11944026/research/ic-ralm-odqa/in-context-ralm/reproduce_retrieval/result/nq-train/formatted-ms2.nq-train.hits-100.json
dev_file: /home/guest/r11944026/research/ic-ralm-odqa/in-context-ralm/reproduce_retrieval/result/nq-dev/formatted-ms2.nq-dev.hits-100.json

## training
# debug: set num_worker=0, pin_memory=False
retriever_model: facebook/contriever # facebook/dpr-ctx_encoder-single-nq-base
lm_model: google/flan-t5-large # gpt2 
model_parallelism: False
train_index_path: embeddings/train_50.pt
dev_index_path: embeddings/dev_10.pt
# train_index_path: embeddings/train_10000.pt # embeddings/contriever/train_10000.pt
# dev_index_path: embeddings/dev_1000.pt # embeddings/contriever/dev_1000.pt
empty_doc_embedding_path: embeddings/empty_doc_embedding.pt
gold_dev_answers_path: results/nq-dev-answers.json
llm_batch_size: 32
num_workers: 0
pin_memory: False
auth_token: None
cache_dir: None
per_device_train_batch_size: 10
per_device_eval_batch_size: 4
adam_eps: 1.0e-8
weight_decay: 0.0
max_grad_norm: 2.0
lr: 2.0e-5
warmup_steps: 1237
max_train_epochs: 5
seed: 19980406
gradient_accumulation_steps: 1
val_check_interval: 1.0
max_round: 1
k: 3
max_tokens: 10
temperature: 0.1