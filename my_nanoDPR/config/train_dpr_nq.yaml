## data
train_file: /home/guest/r11944026/research/ic-ralm-odqa/in-context-ralm/reproduce_retrieval/result/nq-train/formatted-ms2.nq-train.hits-100.json
dev_file: /home/guest/r11944026/research/ic-ralm-odqa/in-context-ralm/reproduce_retrieval/result/nq-dev/formatted-ms2.nq-dev.hits-100.json

## training
# debug: set num_worker=0, pin_memory=False
retriever_model: bert-base-uncased
lm_model: gpt2
model_parallelism: False
train_index_path: embeddings/train_10000.pt
dev_index_path: embeddings/dev_1000.pt
# train_index_path: embeddings/train_full.pt
# dev_index_path: embeddings/dev_full.pt
llm_batch_size: 64
num_workers: 0
pin_memory: False
auth_token: None
cache_dir: None
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
adam_eps: 1.0e-8
weight_decay: 0.0
max_grad_norm: 2.0
lr: 2.0e-5
warmup_steps: 1237
max_train_epochs: 3
seed: 19980406
gradient_accumulation_steps: 1
val_check_interval: 1.0
num_round: 1
k: 1
max_tokens: 10
temperature: 0.1