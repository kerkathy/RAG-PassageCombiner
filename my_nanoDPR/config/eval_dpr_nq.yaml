## data
dev_dir: /home/guest/r11944026/research/ic-ralm-odqa/in-context-ralm/reproduce_retrieval/result/nq-dev
dev_file: formatted-ms2.nq-dev.json
dataset_name: nq
empty_doc: False
data_size: "tune_hp"
ckpt_dir: ckpt
sweep: False

## model
resume_training: 
resume_path:
resume_wandb_id:
doc_encoder_type: contriever # dpr, contriever, bert...
query_encoder_type: contriever # dpr, contriever, bert...
base_index_dir: embeddings
doc_encoder: facebook/contriever  # facebook/dpr-ctx_encoder-single-nq-base , facebook/contriever , google-bert/bert-base-uncased
# query_encoder: wandb/run-20240513_225200-ja1uhk3w/files/step-2506/query_encoder/
query_encoder: facebook/contriever # facebook/contriever , facebook/dpr-question_encoder-single-nq-base , google-bert/bert-base-uncased
lm_model: google/flan-t5-large
# gpt2, google/flan-t5-large, huggyllama/llama-7b, meta-llama/Meta-Llama-3-8B-Instruct
quantized: False
model_parallelism: False
## training
max_round: 1
k: 3
per_device_eval_batch_size: 32 # 8 ok
eval_llm_batch_size: 10
num_exemplars: 3
# debug: set num_worker=0, pin_memory=False
num_workers: 0
pin_memory: False
auth_token: None
cache_dir: cache
max_train_epochs: 5
seed: 19980406
gradient_accumulation_steps: 1
val_check_interval: 1.0
max_tokens_to_generate: 10
temperature: 0.1
# old_query_encoder_path: wandb/run-20240510_020643-s817q8kg
# eval_steps: 2000
# max_eval_steps: 10000
# old_query_encoder_path: wandb/run-20240510_021133-xpwtaz9g
# eval_steps: 2500
# max_eval_steps: 12500
# old_query_encoder_path: wandb/run-20240510_014412-ggch6mbr
# eval_steps: 1000
# max_eval_steps: 5000
old_query_encoder_path: wandb/run-20240511_121518-7uu45k5j
eval_steps: 626
max_eval_steps: 3130